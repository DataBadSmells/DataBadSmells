{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import os\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meta_pkt_df(input_dir, output_dir=None, save=False):\n",
    "    print(f\"[*] Loading Dataset From {input_dir}\")\n",
    "    pkt_df = pd.DataFrame()\n",
    "    meta_df = pd.DataFrame()\n",
    "    temp_df = pd.DataFrame()\n",
    "    for sub_dir in os.listdir(input_dir):\n",
    "        d = os.path.join(input_dir, sub_dir)\n",
    "        if os.path.isdir(d):\n",
    "            for f in os.listdir(d):\n",
    "                if \"csv\" in f:\n",
    "                    name = f.split('.')[0].split('-')[-1]\n",
    "                    if name == 'metadata':\n",
    "                        temp_df = pd.read_csv(os.path.join(d, f), sep=',', header=None)\n",
    "                        temp_df.columns = ['Timestamp', 'Unix Timestamp', 'Source IP', 'Source Port', 'Destination IP', 'Destination Port', 'Protocol']\n",
    "                        meta_df = pd.concat([meta_df, temp_df])\n",
    "                    else:\n",
    "                        temp_df = pd.read_csv(os.path.join(d, f), delimiter='\\t', header=None)\n",
    "                        temp_df.columns = [name]\n",
    "                        pkt_df = pd.concat([pkt_df, temp_df], axis=1)\n",
    "    if save:\n",
    "            meta_df.to_csv(f'./{output_dir}/metadata.csv', index=False)\n",
    "            pkt_df.to_csv(f'./{output_dir}/pkt.csv', index=False)\n",
    "\n",
    "    return meta_df, pkt_df\n",
    "\n",
    "def load_nested_meta_pkt_df(input_dir, output_dir=None, save=False):\n",
    "    print(f\"[*] Loading Dataset From {input_dir}\")\n",
    "    pkt_df = pd.DataFrame()\n",
    "    meta_df = pd.DataFrame()\n",
    "    temp_df = pd.DataFrame()\n",
    "    for sub_dir in os.listdir(input_dir):\n",
    "        d1 = os.path.join(input_dir, sub_dir)\n",
    "        for sub_sub_dir in os.listdir(d1):\n",
    "            d2 = os.path.join(d1, sub_sub_dir)\n",
    "            temp_pkt_df = pd.DataFrame()\n",
    "            if os.path.isdir(d2):\n",
    "                for f in os.listdir(d2):\n",
    "                    if \"csv\" in f:\n",
    "                        name = f.split('.')[0].split('-')[-1]\n",
    "                        if name == 'metadata':\n",
    "                            temp_df = pd.read_csv(os.path.join(d2, f), sep=',', header=None)\n",
    "                            temp_df.columns = ['Timestamp', 'Unix Timestamp', 'Source IP', 'Source Port', 'Destination IP', 'Destination Port', 'Protocol']\n",
    "                            meta_df = pd.concat([meta_df, temp_df])\n",
    "                        else:\n",
    "                            temp_df = pd.read_csv(os.path.join(d2, f), delimiter='\\t', header=None)\n",
    "                            temp_df.columns = [name]\n",
    "                            temp_pkt_df = pd.concat([temp_pkt_df, temp_df], axis=1)\n",
    "            pkt_df = pd.concat([pkt_df, temp_pkt_df])\n",
    "    if save:\n",
    "        meta_df.to_csv(f'./{output_dir}/metadata.csv', index=False)\n",
    "        pkt_df.to_csv(f'./{output_dir}/pkt.csv', index=False)\n",
    "\n",
    "    return meta_df, pkt_df\n",
    "\n",
    "# Merge the columns of the metadata and packet dataframes\n",
    "def combine_meta_pkt_dfs(meta_df, pkt_df):\n",
    "    return pd.concat([meta_df, pkt_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    print(f\"\\t[*] Preprocessing . . .\")\n",
    "    # Sort the DataFrame by 'Source IP', 'Destination IP', and 'Unix Timestamp'\n",
    "    df.sort_values(by=['Source IP', 'Destination IP', 'Unix Timestamp'], inplace=True)\n",
    "    # Replace 6 with 0 and 17 with 1 in the specified column\n",
    "    df['Protocol'] = df['Protocol'].replace({6: 0, 17: 1})\n",
    "    # Convert strings to lists\n",
    "    df[['Pkt_Direction', 'Pkt_Flags', 'Pkt_IATs', 'Pkt_Sizes', \"Pkt_Header_Sizes\"]] = df[['Pkt_Direction', 'Pkt_Flags', 'Pkt_IATs', 'Pkt_Sizes', \"Pkt_Header_Sizes\"]].applymap(literal_eval)\n",
    "    # Drop unwanted columns\n",
    "    df = df[['Protocol', 'Pkt_Direction', 'Pkt_Flags', 'Pkt_IATs', 'Pkt_Sizes', \"Pkt_Header_Sizes\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_datasets():\n",
    "\n",
    "    pos_dir = './data/pos_wednesday.csv'\n",
    "    pos_df = pd.read_csv(pos_dir)\n",
    "    mon_dir = './data/pos_monday.csv'\n",
    "    mon_df = pd.read_csv(mon_dir)\n",
    "\n",
    "    pos_df = pd.concat([pos_df, mon_df])\n",
    "\n",
    "    neg_dir = './data/neg_wednesday.csv' # All DoS Attacks\n",
    "    neg_df = pd.read_csv(neg_dir)\n",
    "\n",
    "    return pos_df, neg_df\n",
    "\n",
    "def load_datasets(resample=None):\n",
    "    pos_df, neg_df = get_datasets()\n",
    "\n",
    "    pos_train_df = pos_df.sample(frac=0.8, random_state=42)\n",
    "    neg_train_df = neg_df.sample(frac=0.8, random_state=42)\n",
    "\n",
    "    pos_test_df = pos_df[~pos_df.index.isin(pos_train_df.index)]\n",
    "    neg_test_df = neg_df[~neg_df.index.isin(neg_train_df.index)]\n",
    "\n",
    "    print(\"\\t[*] Rebalancing data . . .\")\n",
    "    # Drop unwanted rows\n",
    "    if resample != None:\n",
    "        neg_test_df = neg_test_df.sample(frac=resample)\n",
    "    neg_test_df = neg_test_df.reset_index(drop=True)\n",
    "\n",
    "    # Randomly sample from the negative dataframe to match the size of the positive dataframe\n",
    "    # This probably isn't best practise but it's also what LUCID does, so let's leave it in\n",
    "    pos_train_df = pos_train_df.sample(n=len(neg_train_df.index), random_state=42)\n",
    "    pos_test_df = pos_test_df.sample(n=len(neg_test_df.index), random_state=42)\n",
    "\n",
    "    pos_train_df = preprocess(pos_train_df)\n",
    "    neg_train_df = preprocess(neg_train_df)\n",
    "\n",
    "    pos_test_df = preprocess(pos_test_df)\n",
    "    neg_test_df = preprocess(neg_test_df)\n",
    "\n",
    "    return pos_train_df, neg_train_df, pos_test_df, neg_test_df\n",
    "\n",
    "# Function to transform a row with varying length to a row with fixed length (m)\n",
    "def transform_row_from_tuple(row, m):\n",
    "    if len(row) > m:\n",
    "        # Truncate values\n",
    "        row = row[:m - 1]\n",
    "    elif len(row) < m:\n",
    "        # Fill with zeros until the desired length is reached\n",
    "        row += [0] * (m - len(row))\n",
    "    return row\n",
    "\n",
    "\n",
    "def _feature_engineer(df, m=4, columns_to_transform=['Pkt_Direction', 'Pkt_IATs', 'Pkt_Sizes', 'Pkt_Flags', \"Pkt_Header_Sizes\"], save=False):\n",
    "    print(f\"\\t[*] Feature engineering with packet depth {m}\")\n",
    "    for col in columns_to_transform:\n",
    "        df[col] = df[col].apply(lambda x: transform_row_from_tuple(list(x), m) if isinstance(x, (list, tuple)) else transform_row_from_tuple([x], m))\n",
    "        \n",
    "    # Custom function to flatten lists and lists of lists\n",
    "    def flatten_element(cell):\n",
    "        if isinstance(cell, list):\n",
    "            return [item for sublist in cell for item in sublist] if any(isinstance(item, list) for item in cell) else cell\n",
    "        return cell\n",
    "\n",
    "    df = df.applymap(flatten_element)\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_engineer(pos_train_df, neg_train_df, pos_test_df, neg_test_df, m):\n",
    "    pos_train_df = _feature_engineer(pos_train_df, m=m)\n",
    "    neg_train_df = _feature_engineer(neg_train_df, m=m)\n",
    "    pos_test_df = _feature_engineer(pos_test_df, m=m)\n",
    "    neg_test_df = _feature_engineer(neg_test_df, m=m)\n",
    "\n",
    "    return pos_train_df, neg_train_df, pos_test_df, neg_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[*] Rebalancing data . . .\n",
      "\t[*] Preprocessing . . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37948/3720273977.py:8: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[['Pkt_Direction', 'Pkt_Flags', 'Pkt_IATs', 'Pkt_Sizes', \"Pkt_Header_Sizes\"]] = df[['Pkt_Direction', 'Pkt_Flags', 'Pkt_IATs', 'Pkt_Sizes', \"Pkt_Header_Sizes\"]].applymap(literal_eval)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[*] Preprocessing . . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37948/3720273977.py:8: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[['Pkt_Direction', 'Pkt_Flags', 'Pkt_IATs', 'Pkt_Sizes', \"Pkt_Header_Sizes\"]] = df[['Pkt_Direction', 'Pkt_Flags', 'Pkt_IATs', 'Pkt_Sizes', \"Pkt_Header_Sizes\"]].applymap(literal_eval)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[*] Preprocessing . . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37948/3720273977.py:8: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[['Pkt_Direction', 'Pkt_Flags', 'Pkt_IATs', 'Pkt_Sizes', \"Pkt_Header_Sizes\"]] = df[['Pkt_Direction', 'Pkt_Flags', 'Pkt_IATs', 'Pkt_Sizes', \"Pkt_Header_Sizes\"]].applymap(literal_eval)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[*] Preprocessing . . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37948/3720273977.py:8: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[['Pkt_Direction', 'Pkt_Flags', 'Pkt_IATs', 'Pkt_Sizes', \"Pkt_Header_Sizes\"]] = df[['Pkt_Direction', 'Pkt_Flags', 'Pkt_IATs', 'Pkt_Sizes', \"Pkt_Header_Sizes\"]].applymap(literal_eval)\n"
     ]
    }
   ],
   "source": [
    "train_pos, train_neg, test_pos, test_neg = load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[*] Feature engineering with packet depth 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37948/3720273977.py:78: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(flatten_element)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[*] Feature engineering with packet depth 10\n",
      "\t[*] Feature engineering with packet depth 10\n",
      "\t[*] Feature engineering with packet depth 10\n"
     ]
    }
   ],
   "source": [
    "train_pos, train_neg, test_pos, test_neg = feature_engineer(train_pos, train_neg, test_pos, test_neg, m = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale function\n",
    "def scale_column(row, min, max):\n",
    "    if isinstance(row, list):\n",
    "        return [(r - min) / (max - min) if r < max else 1 for r in row ]\n",
    "    else:\n",
    "        return (row - min) / (max - min) if row < max else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_numpy(pos_df, neg_df, save=False):\n",
    "    print(\"\\t[*] Converting to numpy format\")\n",
    "    # Add the labels column\n",
    "    pos_df['label'] = 0\n",
    "    neg_df['label'] = 1\n",
    "\n",
    "    # Concatenate the 2 dataframes\n",
    "    data_df = pd.concat([pos_df, neg_df])\n",
    "\n",
    "    # Save the dataframe\n",
    "    if save:\n",
    "        data_df.to_csv(f'./data/data.csv', index=True)\n",
    "\n",
    "    # Convert 'Pkt_Direction,' 'Pkt_Flags,' 'Pkt_IATs,' and 'Pkt_Sizes' to lists\n",
    "    data_df['Pkt_Direction'] = data_df['Pkt_Direction'].apply(list)\n",
    "    data_df['Pkt_Direction'] = data_df['Pkt_Direction'].apply(sum)\n",
    "\n",
    "    data_df['Pkt_Flags'] = data_df['Pkt_Flags'].apply(list)\n",
    "    #data_df['Pkt_Flags'] = data_df['Pkt_Flags'].apply(sum)\n",
    "\n",
    "    data_df['Pkt_IATs'] = data_df['Pkt_IATs'].apply(list)\n",
    "    data_df['Pkt_IATs'] = data_df['Pkt_IATs'].apply(sum)\n",
    "\n",
    "    data_df['Pkt_Sizes'] = data_df['Pkt_Sizes'].apply(list)\n",
    "    data_df['Pkt_Sizes'] = data_df['Pkt_Sizes'].apply(sum) \n",
    "\n",
    "    data_df[\"Pkt_Header_Sizes\"] = data_df[\"Pkt_Header_Sizes\"].apply(list)\n",
    "    data_df[\"Pkt_Header_Sizes\"] = data_df[\"Pkt_Header_Sizes\"].apply(sum)\n",
    "\n",
    "    # Ensure consistent data types )(float32 for floating-point values, int32 for integers)                                           # This makes it negative, for some reason\n",
    "    data_df['Protocol'] = data_df['Protocol'].astype('int32')\n",
    "    data_df['label'] = data_df['label'].astype('int32')\n",
    "        \n",
    "    # Normalise columns\n",
    "    data_df['Pkt_Sizes'] = data_df['Pkt_Sizes'].apply(scale_column, min=0, max=100000)\n",
    "    data_df[\"Pkt_Header_Sizes\"] = data_df[\"Pkt_Header_Sizes\"].apply(scale_column, min=0, max=10000)\n",
    "    data_df['Pkt_IATs'] = data_df['Pkt_IATs'].apply(scale_column, min=0, max=750000000)                   \n",
    "    data_df['Pkt_Flags'] = data_df['Pkt_Flags'].apply(scale_column, min=0, max=256)                       \n",
    "\n",
    "    if save:\n",
    "        # Save the dataframe\n",
    "        data_df.to_csv(f'./data/data.csv', index=True)\n",
    "\n",
    "    if save:\n",
    "        # Save the dataframe\n",
    "        data_df.to_csv(f'./data/data-2.csv', index=True)\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def to_numpy(pos_train_df, neg_train_df, save=False):\n",
    "    data_df = _to_numpy(pos_train_df,\n",
    "                                         neg_train_df,\n",
    "                                         save)\n",
    "\n",
    "    return data_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[*] Converting to numpy format\n"
     ]
    }
   ],
   "source": [
    "train_df = to_numpy(train_pos, train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[*] Converting to numpy format\n"
     ]
    }
   ],
   "source": [
    "test_df = to_numpy(test_pos, test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids(mode=None, train_features=None, train_labels=None, test_features=None, test_labels=None, random_state=42):\n",
    "    if mode == 'Forest':\n",
    "        classifier = RandomForestClassifier(n_estimators = 1000, random_state = random_state)\n",
    "        print(\"Classifier Initialised\")\n",
    "        classifier.fit(train_features, train_labels)\n",
    "        print(\"Classifier Trained\")\n",
    "        predictions  = classifier.predict(test_features)\n",
    "    elif mode == 'Logistic':\n",
    "        classifier = LogisticRegression(random_state=100, max_iter=1000)\n",
    "        print(\"Classifier Initialised\")\n",
    "        classifier.fit(train_features, train_labels)\n",
    "        print(\"Classifier Trained\")\n",
    "        predictions = classifier.predict(test_features)\n",
    "    elif mode == 'MLP':\n",
    "        classifier = MLPClassifier(random_state = 100, hidden_layer_sizes=(150,100,50), max_iter=300, activation='relu', solver='adam')\n",
    "        print(\"Classifier Initialised\")\n",
    "        classifier.fit(train_features, train_labels)\n",
    "        print(\"Classifier Trained\")\n",
    "        predictions = classifier.predict(test_features)\n",
    "    else:\n",
    "        print('Choose valid mode.')\n",
    "    test_score = np.mean(test_labels == predictions)\n",
    "    \n",
    "    return predictions, test_score, classifier\n",
    "\n",
    "def results(test_score=None, predictions=None, test_labels=None):\n",
    "    print(\"Test Score: \", test_score)\n",
    "    print(confusion_matrix(test_labels.values, predictions))\n",
    "    \n",
    "    print(\"F1 Score:\", f1_score(test_labels.values, predictions, average='macro'))\n",
    "\n",
    "    return f1_score(test_labels.values, predictions, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = False # Make corrections to the dataset by discarding malformed flows from the data. \n",
    "# When corrections is false, F1 score can fluctuate from approx. 0.992 to 0.997, depending on dataset split\n",
    "if corrections:\n",
    "    train_df = train_df.drop(train_df[(train_df[\"Pkt_Sizes\"] < 0.1) & (train_df[\"label\"] == 1)].index) # drop positive flows which consist of flow appendices, based on outliers in size\n",
    "    test_df = test_df.drop(test_df[(test_df[\"Pkt_Header_Sizes\"] == 0) | (test_df[\"Pkt_Header_Sizes\"] == 1)].index) # drop packets with unusual header sizes\n",
    "    test_df = test_df.drop(test_df[(test_df[\"Pkt_IATs\"] == 0) | (test_df[\"Pkt_IATs\"] == 1)].index) # drop test flows with one packet or which timeout\n",
    "    test_df = test_df.drop(test_df[(test_df[\"Pkt_Sizes\"] < 0.1) & (test_df[\"label\"] == 1)].index) # drop positive flows which consist of flow appendices, based on outliers in size\n",
    "    test_df = test_df.drop(test_df[(test_df[\"Pkt_IATs\"] > 0.9)].index) # Drop packets with outlier IATs\n",
    "    test_df = test_df.drop(test_df[(test_df[\"Pkt_Direction\"] < 2)].index) # drop test flows with malformed handshakes (< 2 packets in positive direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Initialised\n",
      "Classifier Trained\n",
      "Test Score:  0.9930033280050313\n",
      "[[38048   113]\n",
      " [  421 37740]]\n",
      "F1 Score: 0.9930032140586504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9930032140586504"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds, score, classifier = ids(\"Forest\", \n",
    "            train_df[[\"Pkt_IATs\", \"Pkt_Sizes\", \"Pkt_Header_Sizes\"]], \n",
    "            train_df[\"label\"], test_df[[\"Pkt_IATs\", \"Pkt_Sizes\", \"Pkt_Header_Sizes\"]],\n",
    "            test_df[\"label\"],\n",
    "            random_state=42)\n",
    "results(score, preds, test_df[\"label\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.11.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
