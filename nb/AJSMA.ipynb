{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c903d09",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "dir_path = os.path.dirname(os.getcwd() + \"/../src/\")\n",
    "sys.path.insert(1, dir_path)\n",
    "import data_manip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0704c18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ac95fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf87ec",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb7d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids(mode=None, train_features=None, train_labels=None, test_features=None, test_labels=None):\n",
    "    if mode == 'Forest':\n",
    "        classifier = RandomForestClassifier(n_estimators = 10, random_state = 100)\n",
    "        print(\"Classifier Initialised\")\n",
    "        classifier.fit(train_features, train_labels)\n",
    "        print(\"Classifier Trained\")\n",
    "        predictions  = classifier.predict(test_features)\n",
    "    elif mode == 'Logistic':\n",
    "        classifier = LogisticRegression(random_state=100, max_iter=1000)\n",
    "        print(\"Classifier Initialised\")\n",
    "        classifier.fit(train_features, train_labels)\n",
    "        print(\"Classifier Trained\")\n",
    "        predictions = classifier.predict(test_features)\n",
    "    elif mode == 'MLP':\n",
    "        classifier = MLPClassifier(random_state=100, hidden_layer_sizes=(64,32), max_iter=1, activation='relu', solver='adam')\n",
    "        print(\"Classifier Initialised\")\n",
    "        classifier.fit(train_features, train_labels)\n",
    "        print(\"Classifier Trained\")\n",
    "        predictions = classifier.predict(test_features)\n",
    "    else:\n",
    "        print('Choose valid mode.')\n",
    "    test_score = np.mean(test_labels == predictions)\n",
    "    \n",
    "    return predictions, test_score, classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3857c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basicFeatureImportance(df=None, classifier=None, name=None):\n",
    "    if name == \"Forest\":\n",
    "        num_feat = min(10,classifier.n_features_)\n",
    "        importances = classifier.feature_importances_\n",
    "        std = np.std([tree.feature_importances_ for tree in classifier.estimators_],\n",
    "             axis=0)\n",
    "        indices = np.argsort(importances)[::-1][:num_feat]\n",
    "\n",
    "        print(\"Feature ranking:\")\n",
    "\n",
    "        for f in range(num_feat):\n",
    "            print(\"feature \",indices[f],\":\", df.columns.values[indices[f]], importances[indices[f]])\n",
    "\n",
    "    \n",
    "        plt.figure()\n",
    "        plt.title(\"Feature importances\")\n",
    "        plt.bar(range(num_feat), importances[indices],\n",
    "            color=\"r\", yerr=std[indices], align=\"center\")\n",
    "        plt.xticks(range(num_feat), indices)\n",
    "        plt.xlim([-1, num_feat])\n",
    "        plt.show()\n",
    "        return df.columns.values[indices]\n",
    "    if name == \"Logistic\":\n",
    "        importance = classifier.coef_[0]\n",
    "        # summarize feature importance\n",
    "        for i,v in enumerate(importance):\n",
    "            print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "        # plot feature importance\n",
    "        plt.bar([x for x in range(len(importance))], importance)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60941570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(test_score=None, predictions=None, test_labels=None):\n",
    "    print(\"Test Score: \", test_score)\n",
    "    print(confusion_matrix(test_labels.values, predictions))\n",
    "    \n",
    "    print(\"F1 Score:\", f1_score(test_labels.values, predictions, average='macro'))\n",
    "\n",
    "    return f1_score(test_labels.values, predictions, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5010653",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "It's not extremely clear how to recreate Sheatsley et al.'s methodology exactly, however, they explicitly state their criteria for considering a transformation to e valid. They constrain features to realisable values based on those that appear in the training data. We'll take their results as given and attempt to produce an attack that also fits this constraint.\n",
    "Next, we load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7b21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsw_metadata_file = \"../metadata/unsw/metadata.json\" # Put path to metadata file here\n",
    "unsw_direc = None # Put path to the containing folder of UNSW NB15 CSV data here\n",
    "if (unsw_metadata_file == None) or (unsw_direc == None):\n",
    "    print(\"[*] Please provide a metadata file and/or path to UNSW NB15 directory\")\n",
    "metadata = data_manip.readMetadata(unsw_metadata_file)\n",
    "df = data_manip.readDirec(unsw_direc, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694fa23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aa344c",
   "metadata": {},
   "source": [
    "### We see a marked difference in the ratio of TCP to UDP traffic in the Benign vs Malicious traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ben_proto_value_counts = df[df[metadata[\"label_field\"]] == metadata[\"benign_label\"]][\"proto\"].value_counts()\n",
    "ratio = ben_proto_value_counts[\"tcp\"]/ben_proto_value_counts[\"udp\"]\n",
    "print(f\"Ratio of TCP to UDP in Benign Traffic {ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de96ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mal_proto_value_counts = df[df[metadata[\"label_field\"]] != metadata[\"benign_label\"]][\"proto\"].value_counts()\n",
    "ratio = mal_proto_value_counts[\"tcp\"]/mal_proto_value_counts[\"udp\"]\n",
    "print(f\"Ratio of TCP to UDP in Malicious Traffic {ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e5444f",
   "metadata": {},
   "source": [
    "### We see a similar difference looking at the ratio of traffic with Windows TTL to Linux TTLs in Benign vs Malicious Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378457e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_benign_ttl = df[df[metadata[\"label_field\"]] == metadata[\"benign_label\"]][[\"sttl\", \"dttl\"]].value_counts()[31, 29]\n",
    "total_benign_length = df[df[metadata[\"label_field\"]] == metadata[\"benign_label\"]].shape[0]\n",
    "print(f\"Ratio of Top Windows TTLs to Other in Benign data: {top_benign_ttl/total_benign_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1d8c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[metadata[\"label_field\"]] != metadata[\"benign_label\"]) & (df[\"synack\"] == 0)][[\"proto\", \"synack\", \"ackdat\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0114ae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[metadata[\"label_field\"]] != metadata[\"benign_label\"]) & (df[\"sttl\"] == 254)][\"proto\"].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f62c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mal_ttl = df[df[metadata[\"label_field\"]] != metadata[\"benign_label\"]][[\"sttl\", \"dttl\"]].value_counts()[254, 0]\n",
    "total_mal_length = df[df[metadata[\"label_field\"]] != metadata[\"benign_label\"]].shape[0]\n",
    "print(f\"Ratio of Top Linux TTLs to Other in Mal data: {top_mal_ttl/total_mal_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb80d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_removal(train_features, train_labels, test_features, test_labels):\n",
    "    \"\"\"\n",
    "    dloss and sloss have some pretty extreme outliers, more so than other features.\n",
    "    \"\"\"\n",
    "    dloss_outliers = train_features[\"dloss\"].quantile(0.97)\n",
    "    sloss_outliers = train_features[\"sloss\"].quantile(0.97)\n",
    "    train_outliers = (train_features[\"dloss\"] < dloss_outliers) & (train_features[\"sloss\"] < sloss_outliers)\n",
    "    train_features = train_features[train_outliers]\n",
    "    train_labels = train_labels[train_outliers]\n",
    "    test_outliers = (test_features[\"dloss\"] < dloss_outliers) & (test_features[\"sloss\"] < sloss_outliers)\n",
    "    test_features = test_features[test_outliers]\n",
    "    test_labels = test_labels[test_outliers]\n",
    "\n",
    "    return train_features, train_labels, test_features, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973ea925",
   "metadata": {},
   "source": [
    "## Define our simple perturbation test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1198f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAdversarialTest():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _importances(self, classifier):\n",
    "        num_feat = min(10, classifier.n_features_in_)\n",
    "        importances = classifier.feature_importances_\n",
    "        std = np.std([tree.feature_importances_ for tree in classifier.estimators_],\n",
    "            axis=0)\n",
    "        indices = np.argsort(importances)[::-1][:num_feat]\n",
    "\n",
    "        return importances, std, indices\n",
    "    \n",
    "    def _first_run(self, train_features, train_labels, test_features, test_labels):\n",
    "        \n",
    "        scaler = preprocessing.StandardScaler().fit(train_features)\n",
    "\n",
    "        train_features = scaler.transform(train_features)\n",
    "        test_features = scaler.transform(test_features)\n",
    "        \n",
    "        predictions, test_score, classifier = ids(\"MLP\",\n",
    "                                                    train_features=train_features,\n",
    "                                                    train_labels=train_labels,\n",
    "                                                    test_features=test_features,\n",
    "                                                    test_labels=test_labels)\n",
    "        results(test_score, predictions, test_labels)\n",
    "        return classifier, train_features, train_labels, test_features, test_labels\n",
    "\n",
    "    def _run(self, classifier, test_features):\n",
    "        predictions = classifier.predict(test_features)\n",
    "        return predictions\n",
    "\n",
    "    def _adv(self, test_features, test_labels):\n",
    "        labels = np.where(test_labels == 1)[0]\n",
    "        test_features = test_features.to_numpy()\n",
    "        # We have to adhere to the constraints set out by Sheatsley et al.\n",
    "        # They define the concept of a 'Primary' feature which forces constraints upon all other features.\n",
    "        # Selecting the Protocol as their primary feature, we can only choose values that appear with that feature.\n",
    "        # So, if no flow exists with, say, Proto = UDP and STTL = 256, then perturbing the STTL of a UDP flow such that it equals 256 is forbidden.\n",
    "        # Importantly, they do not consider the flow's label as a primary feature.\n",
    "        # Between the benign and malicious traffic, all protocols have at least one flow with RTTs = 0 and TTLs = 0\n",
    "        # Making these changes, we achieve an attack with perfect accuracy\n",
    "        #test_features[labels, 0] = 1 # <-------- We can also change the protocol to UDP and still adhere to these constraints\n",
    "        test_features[labels, 27] = 0 # ORIGINAL # Change RTT values \n",
    "        test_features[labels, 28] = 0 # ORIGINAL # Change RTT values\n",
    "        test_features[labels, 29] = 0 # ORIGINAL # Change RTT values\n",
    "        test_features[labels, 6] = 0 # ORIGINAL # dttl (Fixed by TCP/UDP as primary feature)\n",
    "        test_features[labels, 5] = 0 # ORIGINAL # sttl (Fixed by TCP/UDP as primary feature)\n",
    "        return test_features\n",
    "    \n",
    "    def pipeline(self, df, metadata, target_label):\n",
    "        df, labels = data_manip.reformatForML(df, metadata, str(target_label))\n",
    "        print(labels.value_counts())\n",
    "        train_features, train_labels, test_features, test_labels, _, _ = data_manip.getTrainTestFeatures(df, labels)\n",
    "        train_features, train_labels, test_features, test_labels = outlier_removal(train_features, train_labels, test_features, test_labels)\n",
    "        classifier, _, _, _, _ = self._first_run(train_features, train_labels, test_features, test_labels)\n",
    "        modified_test_features = self._adv(test_features, test_labels)\n",
    "        scaler = preprocessing.StandardScaler().fit(train_features)\n",
    "        modified_test_features = scaler.transform(modified_test_features)\n",
    "        predictions = self._run(classifier, modified_test_features)\n",
    "        test_score = np.mean(test_labels == predictions)\n",
    "        misclassified = np.where((predictions == 1) & (test_labels == 1))\n",
    "        results(test_score, predictions, test_labels)\n",
    "        return test_features.iloc[misclassified]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd43d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[metadata[\"label_field\"]] != metadata[\"benign_label\"], metadata[\"label_field\"]] = \"1\"\n",
    "print(df[metadata[\"label_field\"]].value_counts())\n",
    "test = SimpleAdversarialTest()\n",
    "classify_correct = test.pipeline(df, metadata, \"1\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c3f05cff548d27ba543faa078c5eb2301136486ed3656c5f899e784307e6258e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('data-science': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
