{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a51686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cd5d47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e567fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24718199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import os, sys\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "dir_path = os.path.dirname(os.getcwd() + \"/../src/\")\n",
    "sys.path.insert(1, dir_path)\n",
    "import data_manip\n",
    "\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb5b269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'DA'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a5d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reformatForMLADDA(df: pd.DataFrame, drop_fields: list, unique_fields: list, label_field: str, benign_label:str,  target, b_resample, balance=False):\n",
    "    \"\"\"\n",
    "    Reformat DataFrame for Machine Learning algorithms\n",
    "    \"\"\"\n",
    "    df = df.drop(drop_fields, axis=1)\n",
    "    df = df.replace(np.inf, np.nan)\n",
    "    df = df.dropna()\n",
    "    if b_resample != None:\n",
    "        ben_df = df[df[label_field] == benign_label]\n",
    "        ben_df = ben_df.sample(frac=b_resample)\n",
    "        df = df.drop(df[df[label_field] == benign_label].index)\n",
    "        df = pd.concat([ben_df, df])\n",
    "    df = df.drop_duplicates()\n",
    "    if benign_label == None:\n",
    "        df[label_field].loc[(df[label_field] != target)] = 'Other'\n",
    "        label_mapping = {\"Other\": 0,\n",
    "                        target: 1}\n",
    "    elif type(target) == list:\n",
    "        label_index = df[label_field].loc[((df[label_field] != benign_label) & (~df[label_field].isin(target)))].index\n",
    "        df = df.drop(label_index)\n",
    "        unique_labels = df[label_field].unique()\n",
    "        print(unique_labels)\n",
    "        ones = [1 for x in range(len(unique_labels))]\n",
    "        label_mapping = dict(zip(unique_labels, ones))\n",
    "        label_mapping[benign_label] = 0\n",
    "    else:\n",
    "        label_index = df[label_field].loc[((df[label_field] != benign_label) & (df[label_field] != target))].index\n",
    "        df = df.drop(label_index)\n",
    "        label_mapping = {benign_label: 0,\n",
    "                        target: 1}\n",
    "    if balance:\n",
    "        g = df.groupby(label_field)\n",
    "        df = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))\n",
    "    df[label_field] = df[label_field].replace(label_mapping)\n",
    "    labels = df[label_field]\n",
    "    df = df.drop(label_field, axis=1)\n",
    "    if unique_fields is not None:\n",
    "        for field in unique_fields:\n",
    "            unique_field = df[field].unique()\n",
    "            field_mapping = dict(zip(unique_field, range(len(unique_field))))\n",
    "            df = df.replace({field: field_mapping})\n",
    "\n",
    "    return df, labels\n",
    "\n",
    "\n",
    "def reformatForMLADDA(df: pd.DataFrame, metadata: dict, target: str, b_resample: int, balance=False):\n",
    "    \"\"\"\n",
    "    Reformat DataFrame for Machine Learning algorithms\n",
    "    \"\"\"\n",
    "    df, labels = _reformatForMLADDA(df, metadata[\"drop_fields\"], metadata[\"unique_fields\"], metadata[\"label_field\"], metadata['benign_label'], target, b_resample, balance)\n",
    "    return df, labels\n",
    "\n",
    "def reformatForADDA(df, metadata, target, b_resample1, b_resample2, target_limit, target_test_limit, remove=False):\n",
    "    \"\"\"\n",
    "    Reformat data in manner outlined by ADDA, resampling the benign/target traffic where necessary.\n",
    "    \"\"\"\n",
    "    classes = df[metadata[\"label_field\"]].value_counts().index.tolist()\n",
    "    half = int(df.shape[0] / 2)\n",
    "    classes.remove(metadata[\"benign_label\"])\n",
    "    classes.remove(target)\n",
    "    \n",
    "    print(\"Classes: \", classes)\n",
    "    \n",
    "    df1 = df.iloc[:half]\n",
    "    df2 = df.iloc[half:]\n",
    "    del(df)\n",
    "    source, source_labels = reformatForMLADDA(df1, metadata, classes, b_resample1)\n",
    "    if remove:        \n",
    "        # A very large number of attack traffic has the below properties across multiple classes. We remove it to reduce overlap between classes.\n",
    "        # We justify this in more detail below.\n",
    "        source = source.drop(source[(((source[\"spkts\"] == 10) & (source[\"dpkts\"] == 6)) | \n",
    "                                                                                  ((source[\"spkts\"] == 10) & (source[\"dpkts\"] == 8)) |\n",
    "                                                                                 ((source[\"spkts\"] == 2) & (source[\"dpkts\"] == 0)))].index)\n",
    "    source_labels = source_labels.loc[source.index]\n",
    "    print(\"Finished Processing Source\")\n",
    "    full_target, full_target_labels = reformatForMLADDA(df2, metadata, target, b_resample2, balance=True)\n",
    "\n",
    "    if remove:\n",
    "        # Same as above\n",
    "        trimmed_target = full_target.drop(full_target[(((full_target[\"spkts\"] == 10) & (full_target[\"dpkts\"] == 6)) | \n",
    "                                                                                      ((full_target[\"spkts\"] == 10) & (full_target[\"dpkts\"] == 8)) |\n",
    "                                                                                        ((full_target[\"spkts\"] == 2) & (full_target[\"dpkts\"] == 0)))].index)\n",
    "        print(trimmed_target.shape)\n",
    "\n",
    "        trimmed_target = trimmed_target.groupby(metadata[\"label_field\"])\n",
    "        trimmed_target = pd.DataFrame(trimmed_target.apply(lambda x: x.sample(trimmed_target.size().min()).reset_index(drop=True)))\n",
    "        trimmed_target_labels = full_target_labels.loc[trimmed_target.index] # replace with trimmed\n",
    "        target = trimmed_target.sample(n=target_limit, random_state=100)\n",
    "        target_labels = trimmed_target_labels.loc[target.index]\n",
    "        target_test, target_test_labels = trimmed_target.drop(target.index), trimmed_target_labels.drop(target_labels.index)\n",
    "        target_test = target_test.sample(n=target_test_limit, random_state=100)\n",
    "        target_test_labels = target_test_labels.loc[target_test.index]\n",
    "    else:\n",
    "        target = full_target.sample(n=target_limit, random_state=100)\n",
    "        target_labels = full_target_labels.loc[target.index]\n",
    "        target_test, target_test_labels = full_target.drop(target.index), full_target_labels.drop(target_labels.index)\n",
    "        target_test = target_test.sample(n=target_test_limit, random_state=100)\n",
    "        target_test_labels = target_test_labels.loc[target_test.index]\n",
    "    return source, source_labels, target, target_labels, target_test, target_test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bb9e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(TorchDataset):\n",
    "    \"\"\"Characterizes a dataset for PyTorch\"\"\"\n",
    "    def __init__(self, data, labels):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "        self.labels = labels\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generates one sample of data\"\"\"\n",
    "        X = self.data[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f705cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(nn.Module):\n",
    "    \"\"\"\n",
    "        Base Model to compare with\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=26, num_classes=2):\n",
    "        super(Base, self).__init__()\n",
    "        \n",
    "        self.lin1 = nn.Linear(input_size, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.r1 = nn.ReLU(inplace=True)\n",
    "        self.lin2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.r2 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(32, 16)\n",
    "        self.bn3 = nn.BatchNorm1d(16)\n",
    "        self.r3 = nn.ReLU(inplace=True)\n",
    "        self.lin4 = nn.Linear(16, 2)\n",
    "        self.soft = nn.LogSoftmax()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.r1(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.r2(x)\n",
    "        x = self.lin3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.r3(x)\n",
    "        x = self.lin4(x)\n",
    "        pred = self.soft(x)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82365ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "        Generator/Classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=26, num_classes=2):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.lin1 = nn.Linear(input_size, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.r1 = nn.ReLU(inplace=True)\n",
    "        self.lin2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.r2 = nn.ReLU(inplace=True)\n",
    "        self.lin3 = nn.Linear(32, 16)\n",
    "        self.bn3 = nn.BatchNorm1d(16)\n",
    "        self.r3 = nn.ReLU(inplace=True)\n",
    "        self.lin4 = nn.Linear(16, 2)\n",
    "        self.soft = nn.LogSoftmax()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.r1(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.r2(x)\n",
    "        x = self.lin3(x)\n",
    "        x = self.bn3(x)\n",
    "        embed = self.r3(x)\n",
    "        pred = self.lin4(embed)\n",
    "        pred = self.soft(pred)\n",
    "        \n",
    "        return embed, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be254a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "        Simple Discriminator w/ MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=16, num_classes=2):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(16, 2),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        \n",
    "    def forward(self, h):\n",
    "        c = self.layer(h)\n",
    "        return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad98aa1e",
   "metadata": {},
   "source": [
    "The last layer of the\n",
    "generator serves as input to the discriminator as well as feeds into\n",
    "a soft-max layer to predict the class of the data sample. Similarly,\n",
    "the final layer of the discriminator feeds into a soft-max layer to\n",
    "predict the domain the sample belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c87d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenDomainLoss(dg_tgt):\n",
    "    return -(torch.mean(torch.log(dg_tgt[:, :1])))\n",
    "\n",
    "def DiscClassLoss(source_probs, target_probs):\n",
    "    log_source_probs = torch.log(source_probs[:, :1])\n",
    "    log_target_probs = torch.log(1 - target_probs[:, :1])\n",
    "    left = torch.mean(log_source_probs)\n",
    "    right = torch.mean(log_target_probs)\n",
    "    return -(left + right)\n",
    "\n",
    "def set_requires_grad(model, requires_grad=True):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = requires_grad\n",
    "        \n",
    "GenClassLoss = nn.NLLLoss()\n",
    "BaseLoss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3c47b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = True # True: Recreate original pipeline. False: Drop features/flows that are hihgly discriminitive across multiple classes.\n",
    "\n",
    "unsw_direc = None # Put directory containing UNSW NB15 CSV files here\n",
    "target_train_limit = 100\n",
    "target_test_limit = 10000\n",
    "\n",
    "if original:\n",
    "    unsw_metadata_file = \"../metadata/unsw/metadata.json\" # Put original metadata file here i.e., /unsw/metadata.json \n",
    "else:\n",
    "    unsw_metadata_file = \"../metadata/unsw/metadata_adda.json\" #  Put adda metadata file here i.e., /unsw/metadata_adda.json \n",
    "metadata = data_manip.readMetadata(unsw_metadata_file)\n",
    "df = data_manip.readDirec(unsw_direc, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af04c50",
   "metadata": {},
   "source": [
    "We note that, due to the 'strikes' contained within UNSW NB15, Malicious flows are clustered around a small number of flow sizes whilst Benign flows are not. In particular, a model can achieve extremely good cross-class generalisation based on the 'dpkt' and 'spkt' features alone. This occurs across completely distinct attacks where there is no reasonable justification for this generalisation. Other features (ttls, rtts, etc.) are similarly discriminative. As a result, evaluating a model's cross-class generalisation capabilities on UNSW NB15 is difficult to justify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbb1aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if original:\n",
    "    print(df[(((df[\"spkts\"] == 10) & (df[\"dpkts\"] == 6)) |\n",
    "     ((df[\"spkts\"] == 10) & (df[\"dpkts\"] == 8)) |\n",
    "     ((df[\"spkts\"] == 2) & (df[\"dpkts\"] == 0)))][metadata[\"label_field\"]].value_counts() / df[metadata[\"label_field\"]].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6c0270",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if original:\n",
    "    remove=False\n",
    "    benign_resample = 0.05 # We resample the benign traffic to approximately match the original ADDA amounts\n",
    "else:\n",
    "    remove=True\n",
    "    benign_resample = 0.015 # We change the sample rate so the benign:malicious ratio remains approximately the same\n",
    "source, source_labels, target, target_labels, test, test_labels = reformatForADDA(df, metadata, \"Exploits\", benign_resample, .1, target_train_limit, target_test_limit, remove=remove)\n",
    "source, source_labels, target, target_labels, test, test_labels = source.to_numpy(), source_labels.to_numpy(), target.to_numpy(), target_labels.to_numpy(), test.to_numpy(), test_labels.to_numpy()\n",
    "df = df.drop(metadata[\"drop_fields\"], axis=1)\n",
    "input_size = len(df.columns.tolist())\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2f540",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = Base(input_size=input_size).double().to(DEVICE)\n",
    "G = Generator(input_size=input_size).double().to(DEVICE)\n",
    "D = Discriminator().double().to(DEVICE)\n",
    "\n",
    "G_opt = torch.optim.Adam(G.parameters())\n",
    "D_opt = torch.optim.Adam(D.parameters())\n",
    "B_opt = torch.optim.Adam(B.parameters())\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "max_epoch_base = 10000\n",
    "step_adda = 0\n",
    "step_base = 0\n",
    "\n",
    "ll_g, ll_d = [], []\n",
    "acc_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92f0a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper claims to train for 10000 iterations --- we assume that means over the generator training set\n",
    "max_epoch_adda = math.ceil(10000 / (target_train_limit / (batch_size / 2)))\n",
    "print(max_epoch_adda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset = Dataset(source, source_labels) # Define Source Dataset\n",
    "target_dataset = Dataset(target, target_labels) # Define Target Dataset\n",
    "test_dataset = Dataset(test, test_labels)\n",
    "\n",
    "source_dataloader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "target_dataloader = DataLoader(target_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a555cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def adda_pipeline(step_adda):\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        for epoch in range(1, max_epoch_adda+1):\n",
    "            d_loss_avg = []\n",
    "            gd_loss_avg = []\n",
    "            gc_loss_avg = []\n",
    "            for idx, ((src_entries, src_class_labels), (tgt_entries, tgt_class_labels)) in enumerate(zip(source_dataloader, target_dataloader)):\n",
    "\n",
    "                ##########################\n",
    "                # Training Discriminator #\n",
    "                ##########################\n",
    "\n",
    "\n",
    "                # Get Source, Target and Class Labels\n",
    "                src, sc_labels, tgt, tc_labels = src_entries.to(DEVICE), src_class_labels.to(DEVICE), tgt_entries.to(DEVICE), tgt_class_labels.to(DEVICE)\n",
    "                labels = torch.cat([sc_labels, tc_labels], dim=0)\n",
    "\n",
    "\n",
    "                # Freeze Gradients of G\n",
    "                set_requires_grad(G, requires_grad=False)\n",
    "                set_requires_grad(D, requires_grad=True)\n",
    "\n",
    "                # Combine source and target into single batch\n",
    "                d_samples = torch.cat([src, tgt], dim=0)\n",
    "\n",
    "                # Get Source Labels\n",
    "                src_labels = torch.ones_like(src)\n",
    "                tgt_labels = torch.zeros_like(tgt)\n",
    "                source_labels = torch.cat([src_labels, tgt_labels])\n",
    "\n",
    "                # Get embedding and label predictions for all samples\n",
    "                src_outputs, src_label_preds = G(src.double())\n",
    "                tgt_outputs, tgt_label_preds = G(tgt.double())\n",
    "\n",
    "\n",
    "                # Get Source Predictions for all\n",
    "                src_source_preds = D(src_outputs)\n",
    "                tgt_source_preds = D(tgt_outputs)\n",
    "\n",
    "                # Calculate the loss\n",
    "                disc_loss = DiscClassLoss(src_source_preds, tgt_source_preds)\n",
    "\n",
    "                #Zero out gradients\n",
    "                D.zero_grad()\n",
    "\n",
    "                # Backpropogate\n",
    "                disc_loss.backward(retain_graph=True)\n",
    "\n",
    "                # Update Optimiser\n",
    "                D_opt.step() \n",
    "\n",
    "\n",
    "                ######################\n",
    "                # Training Generator #\n",
    "                ######################\n",
    "\n",
    "                set_requires_grad(D, requires_grad=False)\n",
    "                set_requires_grad(G, requires_grad=True)\n",
    "\n",
    "                # Get Target Outputs Only\n",
    "                # tgt_outputs, tgt_labels\n",
    "\n",
    "                # Get Source Prediction of only the Targets\n",
    "                outputs, label_preds = G(d_samples.double())\n",
    "                new_tgt_source_preds = D(tgt_outputs)\n",
    "\n",
    "\n",
    "                # Calc Domain Loss\n",
    "                gen_domain_loss = GenDomainLoss(new_tgt_source_preds)\n",
    "\n",
    "                ## Calc Classification Loss\n",
    "                gen_class_loss = GenClassLoss(label_preds, labels)\n",
    "\n",
    "\n",
    "                # Zero Out Gradients\n",
    "                G.zero_grad()\n",
    "\n",
    "                # Calc combined loss and backpropogate\n",
    "                comb_loss = ((gen_domain_loss + gen_class_loss) / 2)\n",
    "                comb_loss.backward()\n",
    "\n",
    "\n",
    "                # Update Optimiser\n",
    "                G_opt.step()\n",
    "\n",
    "                if (epoch % 50 == 0) and (epoch != 0):\n",
    "                    d_loss_avg.append(disc_loss.item())\n",
    "                    gd_loss_avg.append(gen_domain_loss.item())\n",
    "                    gc_loss_avg.append(gen_class_loss.item())\n",
    "\n",
    "            if (epoch % 50 == 0) and (epoch != 0):\n",
    "                dt = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "                print('Epoch: {}/{}, Step: {}, D Loss: {:.4f}, G Domain Loss: {:.4f}, G Class Loss: {:.4f} ---- {}'.format(epoch, max_epoch_adda, idx, np.mean(d_loss_avg), np.mean(gd_loss_avg), np.mean(gc_loss_avg), dt))\n",
    "                ll_g.append(comb_loss)\n",
    "                ll_d.append(disc_loss)\n",
    "\n",
    "            if (epoch % 200 == 0) and (idx != 0):\n",
    "                print(\"[!] [Src Source Preds]\\n+++++(Want 0 Acc)+++++\\n\", torch.mean(torch.argmax(label_preds, dim = 1).double()))\n",
    "                print(\"[!] [Target Source Preds]\\n++++++(Want 1 Acc)++++++\\n\", torch.mean(torch.argmax(tgt_source_preds, dim=1).double()))\n",
    "                print(\"[=====================================]\")\n",
    "                print(\"[!] Gen Acc: \", 1 - torch.mean(torch.abs(labels - torch.argmax(label_preds, dim = 1)).double()))\n",
    "                print(\"[=====================================]\")\n",
    "\n",
    "            if (epoch % 50 == 0) and (idx != 0):\n",
    "                G.eval()\n",
    "                D.eval()\n",
    "                corrects = torch.zeros(1).to(DEVICE)\n",
    "                for idx, (tgt, labels) in enumerate(test_dataloader):\n",
    "                    tgt, labels = tgt.to(DEVICE), labels.to(DEVICE)\n",
    "                    outputs, label_preds = G(tgt)\n",
    "                    _, preds = torch.max(label_preds, 1)\n",
    "                    corrects += (preds == labels).sum()\n",
    "                acc = corrects.item() / len(test_dataloader.dataset)\n",
    "                print('***** Test Result: {:.4f}, Step: {}'.format(acc, step_adda))\n",
    "                acc_lst.append(acc)\n",
    "\n",
    "                G.train()\n",
    "                D.train()\n",
    "            step_adda += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aab010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_pipeline(step_base):    \n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        for epoch in range(1, max_epoch_base+1):\n",
    "            bc_loss_avg = []\n",
    "            for idx, ((src_entries, src_class_labels), (tgt_entries, tgt_class_labels)) in enumerate(zip(source_dataloader, target_dataloader)):\n",
    "\n",
    "                ##########################\n",
    "                # Training Discriminator #\n",
    "                ##########################\n",
    "\n",
    "\n",
    "                # Get Source, Target and Class Labels\n",
    "                src, sc_labels, tgt, tc_labels = src_entries.to(DEVICE), src_class_labels.to(DEVICE), tgt_entries.to(DEVICE), tgt_class_labels.to(DEVICE)\n",
    "                labels = torch.cat([sc_labels, tc_labels], dim=0)\n",
    "\n",
    "                # Combine source and target into single batch\n",
    "                d_samples = torch.cat([src, tgt], dim=0)\n",
    "\n",
    "\n",
    "                # Get embedding and label predictions for all samples\n",
    "                label_preds = B(d_samples.double())\n",
    "\n",
    "                # Calculate the loss\n",
    "                class_loss = BaseLoss(label_preds, labels)\n",
    "\n",
    "                # Zero Out Gradients\n",
    "                B.zero_grad()\n",
    "\n",
    "                # Send loss backwards\n",
    "                class_loss.backward()\n",
    "\n",
    "                # Update Optimiser\n",
    "                B_opt.step()\n",
    "\n",
    "                if (epoch % 50 == 0) and (epoch != 0):\n",
    "                    bc_loss_avg.append(class_loss.item())\n",
    "\n",
    "            if (epoch % 100 == 0) and (epoch != 0):\n",
    "                dt = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "                print('Epoch: {}/{}, Step: {}, B Loss: {:.4f} ---- {}'.format(epoch, max_epoch_base, idx, np.mean(bc_loss_avg), dt))\n",
    "            if (epoch % 1000 == 0) and (epoch != 0):\n",
    "                print(epoch)\n",
    "\n",
    "                print(\"[=====================================]\")\n",
    "                print(\"[!] Gen Acc: \", 1 - torch.mean(torch.abs(labels - torch.argmax(label_preds, dim = 1)).double()))\n",
    "                print(\"[=====================================]\")\n",
    "\n",
    "\n",
    "            if (epoch % 100 == 0) and (epoch != 0):\n",
    "                B.eval()\n",
    "                corrects = torch.zeros(1).to(DEVICE)\n",
    "                for idx, (tgt, labels) in enumerate(test_dataloader):\n",
    "                    tgt, labels = tgt.to(DEVICE), labels.to(DEVICE)\n",
    "                    label_preds = B(tgt)\n",
    "                    _, preds = torch.max(label_preds, 1)\n",
    "                    corrects += (preds == labels).sum()\n",
    "                acc = corrects.item() / len(test_dataloader.dataset)\n",
    "                print('***** Test Result: {:.4f}, Step: {}'.format(acc, step_base))\n",
    "                acc_lst.append(acc)\n",
    "\n",
    "                B.train()\n",
    "            step_base += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61570855",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "adda_pipeline(step_adda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d89715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "B = Base().double().to(DEVICE)\n",
    "B_opt = torch.optim.Adam(B.parameters())\n",
    "base_pipeline(step_base)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
